<app-post-body [post]="this.post" [date]="this.post.date">
  <p>Way back when (in the spring of 2021), I was a student in my final year of my Bachelor's degree, wherein I had to do a Bachelor's project.</p>

  <p>This project, which I took up mainly because I had to, was on a topic proposed by Laurent Moccozet, one of my professors, mainly due to the fact I was unmotivated to come up with a fun project idea.</p>

  <p>In essentia, it revolved around the visualisation of Dominant Language Constellations, which are the focus of a number of academic works authored by <a target="_blank" href="https://www.researchgate.net/profile/Larissa-Aronin"><em class="fas fa-external-link-square-alt"></em> Larissa Aronin</a>:</p>

  <app-collapsible buttonText="Specific papers" [active]="false">
    <ul>
      <li>L. Aronin, "Multi-competence and Dominant Language Constellation" in <em>The Cambridge Handbook of Linguistic Multi-Competence</em>, 2016, doi: <a target="_blank" href="https://doi.org/10.1017/CBO9781107425965.007"><em class="fas fa-external-link-square-alt"></em> 10.1017/CBO9781107425965.007</a></li>
      <li>L. Aronin, "Dominant Language Constellations In Education, Language Teaching And Multilingualism", 2018, [online]: <a target="_blank" href="https://ecspm.org/wp-content/uploads/2018/10/Aronin-Larissa.pdf"><em class="fas fa-external-link-square-alt"></em> Personal website</a></li>
      <li>J. Bianco and L. Aronin, "Introduction: The Dominant Language Constellations: A New Perspective on Multilingualism", 2020, doi: <a target="_blank" href="https://doi.org/10.1007/978-3-030-52336-7_1"><em class="fas fa-external-link-square-alt"></em> 10.1007/978-3-030-52336-7_1</a></li>
    </ul>
  </app-collapsible>
  <br/>
  <br/>

  <p>The tool was created as part of an academic collaboration between Larissa Aronin and <a target="_blank" href="https://www.researchgate.net/profile/Laurent-Moccozet"><em class="fas fa-external-link-square-alt"></em> Laurent Moccozet</a>, a lecturer and researcher at the University of Geneva, and myself.</p>

  <p>This blog post explains how it was developed and goes into some of the meatier technical details.</p>

  <h4>Summary</h4>
  <ul>
    <li><a class="no-underline" routerLink="." fragment="the-research-problem">The "Research Problem"</a></li>
    <li><a class="no-underline" routerLink="." fragment="representation-questions">Representation questions</a></li>
    <li><a class="no-underline" routerLink="." fragment="initial-implementation">Initial implementation</a></li>
    <li><a class="no-underline" routerLink="." fragment="problems">Problems</a></li>
    <li><a class="no-underline" routerLink="." fragment="revamped-implementation">Revamped implementation</a></li>
    <li><a class="no-underline" routerLink="." fragment="final-product">Final project</a></li>
  </ul>

  <hr><br/>
  <h4 id="the-research-problem">The "Research Problem"</h4>
  <p>The first questions you may be asking are "well, what exactly is a Dominant Language Constellation?", and "why is it important?".</p>
  <p>To the former, I can answer by saying that a Dominant Language Constellation (or <strong>DLC</strong>) is a "snapshot" representation of the languages a person knows, augmented with a quantification of their perception of the distance between these languages.</p>
  <p class="mb-3">Explained differently, it is a snapshot of a person's vehicle linguistic abilities at a given time.</p>

  <p class="mb-3">The latter question is then answered by stating that this perceived distance has ties to factors influenced by a person's context (as it may change over time), leading to shifts in globalization and of superdiversity, but also of geography, society, work, and community, pointing to an interconnectedness of global and local contexts, which is a phenomenon named after the contraction of the words globalization and localization: <em>glocalization</em>.</p>

  <p>The main issue arises when you attempt to represent a DLC, which researchers had taken to doing with sticks and papier-mach√© balls:</p>
  <div class="media-container">
    <img  src="/assets/images/2023/230309-bsc-project/initial-vis.png">
  </div>

  <p class="mb-3">Although this method enables a tangible representation of the DLC, it does not allow one to systematically compare DLCs, nor does it allow for large studies without a significant amount of organization.</p>

  <p>Due to the potential n-dimensionality of a DLC, representing it as a 2D render on a computer is also not very practical (see below), because the notions of depth and interactability are lost.</p>
  <div class="media-container">
    <img  src="/assets/images/2023/230309-bsc-project/virtual-vis.png">
  </div>

  <p>To summarize, this project needed an accessible way to visualize and compare DLCs, which can be interacted with and present the notion of depth (ie. 3D representation).</p>
  <p>On the technical side, this means that we need an interface (preferably in a browser) that has the ability to display data collected from any respondent to the experiment, implying that it is stored in a database of some kind.</p>

  <hr><br/>
  <h4 id="representation-questions">Representation questions</h4>
  <p>Before getting into specific technologies, I needed to consider the various use cases (diagram below), how the information would be collected, what would be an efficient way to represent it as information, and how the various services would communicate.</p>

  <div class="media-container">
    <img  src="/assets/images/2023/230309-bsc-project/use-cases.png">
  </div>

  <p>For simplicity, we opted to collect information via Google Forms, which is a bit iffy from a privacy perspective, but due to the information collected we were told did not constitute a problem.</p>

  <ul>
    <li>Respondent Details:</li>
    <ul>
      <li>Age</li>
      <li>Gender</li>
      <li>Nationality</li>
      <li>Country of Residence</li>
      <li>Level of Education</li>
    </ul>
    <li>Languages spoken:</li>
    <ul>
      <li>Language name and spoken dialect</li>
      <li>Proficiency</li>
      <li>Familiarity</li>
    </ul>
    <li>Self-assessed linguistic "distance"</li>
  </ul>

  <p>We could then move on to conceiving and implementing this service.</p>

  <hr><br/>
  <h4 id="initial-implementation">Initial implementation</h4>

  <p>Since we now had data from 51 respondents, I had to figure out how one would effectively be able to exploit the relationships between the two node types that were available to me (<code>Person</code> and <code>Language</code>).</p>
  <p>Thankfully, I had recently seen graph databases in a class, and decided that using <a target="_blank" href="https://neo4j.com/"><em class="fas fa-external-link-square-alt"></em> Neo4j</a> would be an interesting experiment.</p>

  <br/>
  <h5>Database</h5>

  <p>So I came up with a database schema that would make use of the graph-type relationships that were available:</p>

  <div class="media-container">
    <img  src="/assets/images/2023/230309-bsc-project/nomnoml.svg">
  </div>

  <p>Here, a <code>Person</code> has a relationship called <code>:KNOWS_LANGUAGE</code> with a <code>Language</code>. Additionally, two <code>Language</code> nodes have a relationship specifically with relation to a <code>Person</code>, which is called <code>:HAS_DISTANCE</code>.</p>

  <p>The following representation is not really "standard" but you can see two people (Blue and Green) which know a number of different languages, and the relationship between these languages that carries the "distance" is also color-coded Blue or Green depending on whom the relationship references.</p>

  <div class="media-container">
    <img  src="/assets/images/2023/230309-bsc-project/graph-profiles.png">
  </div>

  <p>Since our form produced a CSV file, we first had to convert the CSV into a series of Cypher queries, which would be done using a Python script. (Cypher being the language used to query Neo4j)</p>

  <p>If you really want to take a look at the (ugly) Python script, you can check it out here: <a target="_blank" href="/assets/images/2023/230309-bsc-project/extractor.py"><em class="fab fa-python"></em> &nbsp;<code>extractor.py</code></a>.</p>

  <p>The Cypher output then looks like below, and can be injected into a Neo4j database using the web-interface or a CLI interface such as <a target="_blank" href="https://github.com/nicolewhite/cycli"><em class="fas fa-external-link-square-alt"></em> cycli</a>.</p>

  <div class="media-container">
    <img  src="/assets/images/2023/230309-bsc-project/cypher-output.png">
  </div>

  <p>This in turn produces a database which is represented like such:</p>

  <div class="media-container">
    <img  src="/assets/images/2023/230309-bsc-project/neo4j-graph.svg">
  </div>

  <br/>
  <h5>Backend</h5>

  <p>Now that we had a database, I could write the backend in Node.js and express, simply because a database connector existed for Neo4j in that environment, and it is quite easy to write a passthru API in it.</p>

  <div class="media-container">
    <img  src="/assets/images/2023/230309-bsc-project/api.png">
  </div>
  <br/>
  <h5>Frontend</h5>

  <p>With a database, a backend, and time pressing, I then started building the frontend in Angular.</p>

  <p>This brought me to the main question: How does one represent a 3D model in a browser and have the ability to interact with it?</p>

  <p>The answer, as it turns out, was force graphs! The main reason being that the force dynamics settle, allowing for the "closest" possible match to the expected model.</p>

  <p>And thankfully, a fork of <code>d3</code> exists to do exactly this: <a target="_blank" href="https://github.com/vasturiano/d3-force-3d"><em class="fas fa-external-link-square-alt"></em> d3-force-3d</a>.</p>

  <div class="media-container">
    <img  src="/assets/images/2023/230309-bsc-project/3d-model.png">
  </div>

  <p>Now all that remained was to make these graphs react based on provided input, and display it to the user:</p>

  <div class="media-container">
    <img  src="/assets/images/2023/230309-bsc-project/frontend-init.png">
  </div>

  <p>Here, the aside allows one to access the various functionalities, the data pane provides the ability to comprehend the raw data, and the graphs pane provides the ability to interact with the rendered model.</p>

  <p>A few features were devised, namely visualizing a single DLC (above), a comparative DLC (below - first), a self-design interface, or the overall DLC cluster (below - second) (or a sub-cluster thereof, below - third).</p>

  <p>What is a DLC cluster? Well it is the averaged DLC of every single respondent, and the sub-cluster only focuses on the languages that connect to a set of specified languages.</p>

  <div class="media-container">
    <img  src="/assets/images/2023/230309-bsc-project/comparison.png">
  </div>

  <div class="media-container">
    <img  src="/assets/images/2023/230309-bsc-project/cluster.png">
  </div>

  <div class="media-container">
    <img  src="/assets/images/2023/230309-bsc-project/subcluster.png">
  </div>

  <hr><br/>
  <h4 id="problems">Problems</h4>

  <p>At this time, I had then presented my work and received my grade, after which the project may or may not have lain dormant for 1.5 years during which I did my M. Sc.</p>

  <p>After defending my M. Sc. thesis (see the <a href='/post/2023/2023_02_15+msc-fun' target="_blank"><em class="fas fa-external-link-square-alt"></em> related blogpost</a>), and considering the overall study was still active, I decided to hand over the project to Laurent Moccozet.</p>

  <p>However this means a few things needed to be fixed, namely:</p>
  <ul>
    <li>The ability for Laurent to be able to add datasets;</li>
    <li>The ability for users to switch datasets and see more information on the service page;</li>
    <li>Using a faster database because of lag on certain requests (Neo4j is Java, after all);</li>
    <li>Switching the API to something less finicky than raw Node.js and Express;</li>
    <li>Having a form of deployment architecture for it to be deployed at the University of Geneva.</li>
  </ul>

  <hr><br/>
  <h4 id="revamped-implementation">Revamped implementation</h4>

  <h5>Database</h5>
  <p>The first thing that I dealt with was attempting to find a competitor for Neo4j that also featured graphs with a similar query language, but in the end was also faster.</p>

  <p>This led me to <a target="_blank" href="https://www.arangodb.com/"><em class="fas fa-external-link-square-alt"></em> ArangoDB</a>, which uses <code>aql</code> for queries, is graph-based, but most importanly is written in C++ with the interaction layer being written in JavaScript.</p>

  <p>This however meant I needed to rewrite the extractor, to convert to <code>aql</code> instead of Cypher, and I also took the liberty to make the output dataset dependent, so more than one dataset can exist concurrently on the service with no collisions and minimal issues to usability.</p>

  <p>If you (again) really want to take a look at the (ugly) Python script, you can check it out here: <a target="_blank" href="/assets/images/2023/230309-bsc-project/extractorarango.py"><em class="fab fa-python"></em> &nbsp;<code>extractorarango.py</code></a>.</p>

  <p>This file, instead of generating one massive Cypher query, builds up the database structure and segments it into CSV files representing the various collections and relationships, hence there being a <code>people-DATASETNAME.csv</code>, a <code>knowsLanguage-DATASETNAME.csv</code>, etc.</p>

  <p>The CSV files contain specific headers that specify nodes (<code>_key</code>) or relationships (<code>_from</code> and <code>_to</code>), with any other columns specifying other attributes of said nodes or relationships.</p>

  <br/>
  <h5>Backend</h5>

  <p>I also ended up rewriting the backend in <a target="_blank" href="https://loopback.io/"><em class="fas fa-external-link-square-alt"></em> Loopback</a>, a TypeScript "wrapper" around Express and Node.js, which functions a lot like FastAPI does for Python.</p>

  <p>I also added the ability to filter by specified dataset, which would enable the API to only respond in the scope of the users request currently selected dataset.</p>

  <br/>
  <h5>Frontend</h5>

  <p>The frontend is the aspect which required the most tweaking and time, but not really from a technical perspective.</p>

  <p>It simply required some additional content, as well as to fix a few CSS issues that were messing up the styling in certain conditions.</p>

  <p>In the aside, I added the dataset switcher, which is simply a dropdown to select a different dataset leading to the dataset to be updated.</p>

  <div class="media-container">
    <img  src="/assets/images/2023/230309-bsc-project/aside-new.png">
  </div>

  <p>Since all of the content requires an initial communication to reach the database, there is now a small loading screen, which was not necessary <em>per se</em>, but was nice to add.</p>

  <div class="media-container">
    <img  src="/assets/images/2023/230309-bsc-project/loading.png">
  </div>

  <p>I then also added an information page which presents the project for readers, and an overview page which shows a number of statistics related to the currently selected dataset.</p>

  <div class="media-container">
    <img  src="/assets/images/2023/230309-bsc-project/stats.png">
  </div>

  <br/>
  <h5>Architecture and Deployment</h5>

  <p>After all of these different bits were solved, I could move on to the deployment of the application.</p>

  <p>Because I enjoy having the ability to replicate things and I absolutely hate developing on barebones infrastructure, I set out to have a Docker deployment of the entire application.</p>

  <p>This led me to initially set up a few containers for the database, the database's persistence, the backend and the frontend.</p>

  <br>
  <h5>Database</h5>

  <p>The database container was most confusing to set up, specifically because it deals with pre-populated data generated by our Python script.</p>

  <p>The first step was to create our Dockerfile:</p>

  <markdown ngPreserveWhitespaces>
```docker
FROM arangodb/arangodb:3.8.8

RUN mkdir -p /tmp /tmp/dataset

WORKDIR /

COPY docker-resources/arangodb.docker-init.sh /arangodb.docker-init.sh
COPY docker-resources/arangodb.docker-init.js /tmp/arangodb.docker-init.js
COPY docker-resources/commands.sh /commands.sh
COPY docker-resources/dataset/* /tmp/dataset/
RUN chmod 0774 /commands.sh
RUN sed -i -e 's/\r$//' /entrypoint.sh
RUN sed -i -e 's/\r$//' /commands.sh
RUN sed -i -e 's/\r$//' /arangodb.docker-init.sh
VOLUME /data

ENTRYPOINT [ "/commands.sh" ]
```
  </markdown>

  <br>

  <p>The <code>commands.sh</code> file contains the following, which allows the <code>arangod</code> (ArangoDB daemon) to start before trying to insert the database contents.</p>

  <markdown ngPreserveWhitespaces>
```shell
#!/bin/sh

(sleep 10; /bin/sh /arangodb.docker-init.sh) &

exec /entrypoint.sh arangod
```
  </markdown>

  <p>The <code>arango.docker-init.sh</code> file is where things get a little... funky.</p>

  <p>As you can see below, it first checks whether or not it has been run (based on the persistence volume)</p>

  <p>If the script has not yet been run, it uses <code>arangosh</code> (the arango shell) to execute a JavaScript file.</p>

  <p>It then loops over all of the CSV files generated by <code>extractorarango.py</code> (described above), and raw imports the collection based on the file name.</p>

  <markdown ngPreserveWhitespaces>
```shell
#!/bin/sh

if [ ! -f "/data/ran_once" ]; then
	echo -e 'Initial Database setup\n';
	/usr/bin/arangosh --server.username root --server.password $ARANGO_ROOT_PASSWORD --javascript.execute /tmp/arangodb.docker-init.js;
	echo -e 'Initial Database setup done\n\n';

	echo -e 'Initial Dataset integration\n';
	for i in $(find /tmp/dataset -name 'people*.csv' | xargs --no-run-if-empty ls | sed -e "s/\/tmp\/dataset\///"  -e "s/.csv//"); do
		usr/bin/arangoimport --file /tmp/dataset/$i.csv --collection $i --create-collection true --type csv --server.database $ARANGO_DBNAME --server.username $ARANGO_UNAME --server.password $ARANGO_PASSWORD;
	done;

  # Additional arangoimport statements

	echo -e 'Initial Dataset integration done\n\n';
	touch /data/ran_once
fi
```
  </markdown>
  <br/>
  <p>The <code>/tmp/arangodb.docker-init.js</code> (referenced above) sets up the credentials to be used and the initial database, based on the environment variables defined in the <code>docker-compose.yaml</code> file shown below.</p>

  <markdown ngPreserveWhitespaces>
```js
const users = require("@arangodb/users");

if (!users.exists(process.env.ARANGO_UNAME)) {{ '{' }}
	users.save(process.env.ARANGO_UNAME, process.env.ARANGO_PASSWORD);
{{ '}' }}

try {{ '{' }}
	db._useDatabase(process.env.ARANGO_DBNAME);
{{ '}' }} catch (e) {{ '{' }}
	db._createDatabase(process.env.ARANGO_DBNAME);
	db._useDatabase(process.env.ARANGO_DBNAME);
	users.grantDatabase(process.env.ARANGO_UNAME, process.env.ARANGO_DBNAME, 'rw');
	users.grantCollection(process.env.ARANGO_UNAME, process.env.ARANGO_DBNAME, '*', 'rw');
{{ '}' }}
```
  </markdown>
  <br/>
  <p>After <em>*waves*</em> all of <em>*that*</em>, we simply specify the database and persistence containers in the <code>docker-compose.yaml</code> file, which is below but collapsed due to its length.</p>

  <app-collapsible [buttonText]="'docker-compose.yaml'" [active]="false">
    <markdown ngPreserveWhitespaces>
```yaml
version: '3'
services:
  lpd-arangodb:
    build:
      context: .
      dockerfile: Dockerfile.db
    container_name: lpd-arangodb
    hostname: lpd-arangodb
    restart: unless-stopped
    environment:
      ARANGO_ROOT_PASSWORD: [REDACTED]
      ARANGO_UNAME: [REDACTED]
      ARANGO_PASSWORD: [REDACTED]
      ARANGO_DBNAME: [REDACTED]
    networks:
      - backend-network
    ports:
      - '8529:8529'
    volumes_from:
      - lpd-arangodb-persistence
    depends_on:
      - lpd-arangodb-persistence

  lpd-arangodb-persistence:
    image: tianon/true
    container_name: lpd-arangodb-persistence
    hostname: lpd-arangodb-persistence
    volumes:
      - './data:/data'

volumes:
  lpd-data:
    name: lpd-arangodb-persistence

networks:
  backend-network:
    driver: bridge
```
    </markdown>
  </app-collapsible>

  <br/>
  <br/>
  <h5>Backend</h5>

  <p>The backend itself is much simpler to build and setup, and simply relies on the following Dockerfile.</p>

  <markdown ngPreserveWhitespaces>
```docker
FROM node:18-slim

USER node

RUN mkdir -p /home/node/app

WORKDIR /home/node/app

COPY --chown=node package*.json ./

RUN npm install --verbose

COPY --chown=node . .

RUN npm run build

ENV HOST=0.0.0.0 PORT=3000

EXPOSE ${{ '{' }}PORT{{ '}' }}
CMD [ "node", "." ]
```
  </markdown>
  <br/>
  <p>We then link the container in the <code>docker-compose.yaml</code>:</p>

  <app-collapsible [buttonText]="'docker-compose.yaml'" [active]="false">
    <markdown ngPreserveWhitespaces>
```yaml
version: '3'
services:
  lpd-arangodb:
    # ...

  lpd-arangodb-persistence:
    # ...

  lpd-backend:
    build:
      context: lpd-backend
      dockerfile: Dockerfile
    container_name: lpd-backend
    hostname: lpd-backend
    restart: unless-stopped
    ports:
      - '1337:3000'
    environment:
      ARANGO_PORT: 8529
      ARANGO_UNAME: [REDACTED]
      ARANGO_PASSWORD: [REDACTED]
      ARANGO_DBNAME: [REDACTED]
      ARANGO_DBURL: lpd-arangodb
    networks:
      - backend-network
    depends_on:
      - lpd-arangodb

volumes:
  # ...

networks:
  backend-network:
    driver: bridge
```
    </markdown>
  </app-collapsible>

  <br/><br>
  <h5>Frontend</h5>

  <p>The frontend is <em>*even simpler*</em> to deal with, since it is built using Angular, one simply needs to generate a production build and throw that into a directory to be served by the server.</p>

  <p>The Dockerfile then looks like this:</p>

  <markdown ngPreserveWhitespaces>
```docker
FROM nginx
COPY nginx.conf /etc/nginx/nginx.conf

COPY ./dist/lpd-frontend/. /usr/share/nginx/html
```
  </markdown>
  <br/>
  <p>The <code>nginx.conf</code> file is simply in charge of serving content on port 80:</p>

  <markdown ngPreserveWhitespaces>
```conf
worker_processes 1;

events {{ '{' }}
  worker_connections 1024;
  accept_mutex off;
  use epoll;
{{'}'}}

http {{ '{' }}
  server {{ '{' }}
    listen 80;
    root /usr/share/nginx/html;
    index index.html;
    location / {{ '{' }}
      try_files $uri $uri/ /index.html;
    {{'}'}}

    location ~* ^.+\.css$ {{ '{' }}
      default_type text/css;
      add_header  Content-Type    text/css;
    {{'}'}}

    # Same as above for .html
    # Same as above for .scss
    # Same as above for .js
  {{'}'}}
{{'}'}}

```
  </markdown>
  <br/>
  <p>We then link the container in the <code>docker-compose.yaml</code>:</p>

  <app-collapsible [buttonText]="'docker-compose.yaml'" [active]="false">
    <markdown ngPreserveWhitespaces>
```yaml
version: '3'
services:
  lpd-arangodb:
    # ...

  lpd-arangodb-persistence:
    # ...

  lpd-backend:
    # ...

  lpd-frontend:
    build:
      context: lpd-frontend
      dockerfile: Dockerfile
    container_name: lpd-frontend
    hostname: lpd-frontend
    restart: unless-stopped
    networks:
      - frontend-network
    depends_on:
      - lpd-backend

volumes:
  # ...

networks:
  backend-network:
    # ...
  frontend-network:
    driver: bridge
```
    </markdown>
  </app-collapsible>

  <br/><br>
  <h5>Linking the bits</h5>

  <p>So we now have a database, an API that can talk to it and a frontend.</p>

  <p>But if you have been paying attention, you may have noticed that the frontend and the backend cannot talk to one another, which may seem counterproductive.</p>

  <p>The reason for this, is that I have time and time again run into issues with CORS policies (Cross-Origin Resource Sharing) even on services hosted on the same server.</p>

  <p>(And most people that have ever done projects like this have <strong>strong</strong> opinions on CORS, source: trust me)</p>

  <p>And since I hate CORS issues, the easiest way I have found to deal with all of this is simply to slap an NGINX proxy in front of it all, with <code>/URI</code> pointing to the frontend, and <code>/api/URI</code> pointing to the API.</p>

  <p>Same origin, same resource, no CORS issues, bliss.</p>

  <p>So we have yet another Dockerfile:</p>

  <markdown ngPreserveWhitespaces>
```docker
FROM nginx
RUN apt-get update

COPY nginx.conf /etc/nginx/nginx.conf
```
  </markdown>
  <br/>
  <p>And yet another <code>nginx.conf</code> file below.</p>

  <p>The core aspects are that it configures the upstreams to point to the frontend and backend and their ports using the Docker DNS hosts resolution (<code>CONTAINER_HOSTNAME:PORT</code>), provides the <code>.acme-challenge</code> for all of the SSL purposes, and redirects to the https website, which, since this is done at the proxy level, won't hinder our accessing the API (because the routing is post SSL!).</p>

  <app-collapsible class="media-container" [buttonText]="'nginx.conf'" [active]="false">
    <markdown ngPreserveWhitespaces>
```conf
worker_processes 32;

events {{ '{' }}
  worker_connections 1024;
  accept_mutex off;
  use epoll;
{{'}'}}

http {{ '{' }}
  include mime.types;
  types {{ '{' }}
    text/scss                             scss;
  {{'}'}}

  client_max_body_size 100M;

  upstream app_serve {{ '{' }}
    server lpd-frontend:80;
  {{'}'}}
  upstream api_serve {{ '{' }}
    server lpd-backend:3000;
  {{'}'}}
  access_log /var/log/nginx/access.log;
  error_log /var/log/nginx/error.log;

  server {{ '{' }}
    listen 80;
    server_name [REDACTED];
    server_tokens off;

    location /.well-known/acme-challenge/ {{ '{' }}
      root /var/www/certbot;
    {{'}'}}

    location / {{ '{' }}
      return 301 https://[REDACTED]$request_uri;
    {{'}'}}
  {{'}'}}

  server {{ '{' }}
    listen 443 default_server ssl http2;
    server_name [REDACTED];

    ssl_certificate /etc/nginx/ssl/live/example.org/fullchain.pem;
    ssl_certificate_key /etc/nginx/ssl/live/example.org/privkey.pem;

    index index.html;
    location ~ ^/api/?((?<=/).*)?$ {{ '{' }}
      proxy_pass http://api_serve/$1$is_args$args;
      proxy_set_header Host               $host;
      proxy_set_header X-Real-IP          $remote_addr;
      proxy_set_header X-Forwarded-For    $proxy_add_x_forwarded_for;
      proxy_set_header X-Forwarded-Proto  $scheme;
    {{'}'}}

    location / {{ '{' }}
      proxy_pass http://app_serve/;
      proxy_set_header Host               $host;
      proxy_set_header X-Real-IP          $remote_addr;
      proxy_set_header X-Forwarded-For    $proxy_add_x_forwarded_for;
      proxy_set_header X-Forwarded-Proto  $scheme;
    {{'}'}}
  {{'}'}}
{{'}'}}
```
    </markdown>
  </app-collapsible>

  <p>"Wait a minute!", you may be asking, "what's all this SSL stuff doing there? We haven't talked about that yet!"</p>

  <p>You are correct!</p>

  <p>As it turns out, the container I set up with the Dockerfile has volumes tacked on by docker-compose that originate from another container that is the <code>certbot</code> container, which allows us to request SSL certifications issued by <a target="_blank" href="https://letsencrypt.org/"><em class="fas fa-external-link-square-alt"></em> Let's Encrypt</a>.</p>


  <p>We then link the containers in the <code>docker-compose.yaml</code>:</p>

  <app-collapsible [buttonText]="'docker-compose.yaml'" [active]="false">
    <markdown ngPreserveWhitespaces>
```yaml
version: '3'
services:
  lpd-arangodb:
    # ...

  lpd-arangodb-persistence:
    # ...

  lpd-backend:
    # ...

  lpd-frontend:
    # ...

  nginx-proxy:
    container_name: nginx-proxy
    restart: unless-stopped
    build:
      context: .
      dockerfile: nginx.Dockerfile
    ports:
      - '80:80'
      - '443:443'
    networks:
      - backend-network
      - frontend-network
    depends_on:
      - lpd-backend
      - lpd-frontend
      - lpd-certbot
    volumes:
      - ./certbot/www:/var/www/certbot/:ro
      - ./certbot/conf/:/etc/nginx/ssl/:ro

  lpd-certbot:
    container_name: lpd-certbot
    image: certbot/certbot:latest
    volumes:
      - ./certbot/www/:/var/www/certbot/:rw
      - ./certbot/conf/:/etc/letsencrypt/:rw
    networks:
      - frontend-network

volumes:
  # ...

networks:
  # ...
```
    </markdown>
  </app-collapsible>

  <br/><br>

  <p>At the end, our architecture looks like this:</p>


  <div class="media-container">
    <img  src="/assets/images/2023/230309-bsc-project/lpd-container.png">
  </div>

  <p>So we now have an entire bunch of services that all work magically. My only remaining issue is the ArangoDB instance is still accessible from the outside on port 8529, which I typically consider to be a bad thing.</p>

  <p>Thankfully, <code>iptables</code>, despite being a pain to <em>not break in eldritch manners</em>, is quite useful for this:</p>

  <markdown ngPreserveWhitespaces>
```shell
sudo iptables -I DOCKER-USER --protocol tcp --dport 8529 -i ens3 --jump REJECT
```
  </markdown>



  <hr><br/>
  <h4 id="final-product">Final project</h4>

  <p>Once I've gotten the go-ahead, I'll unredact the links so you can try it out for yourselves!</p>
</app-post-body>
