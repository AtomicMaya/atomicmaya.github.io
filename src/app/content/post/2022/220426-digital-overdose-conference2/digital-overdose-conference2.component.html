<app-post-body [post]="this.post" [date]="this.post.date">
<p>Once more, we find ourselves at the end of a chapter of a long and ongoing adventure!</p>

<p>If you missed it, I am one of the managers for a community called Digital Overdose (which you can find at <a href='https://digitaloverdose.tech' target='_blank'><em class='fas fa-external-link-square-alt'></em> this link</a>).</p>

<p>I do a lot of things at D.O., from managing day-to-day events, maintaining the website, organizing and running their Capture The Flag events, and finally also running the Digital Overdose Conference (<em>dun-dun-dunnnnn"... pause for dramatic effect</em>).</p>

<p>Well, this year, I organized the second ever Digital Overdose Conference, and I wanted to give you all a bit of a behind the scenes look as to how one organizes a fully virtual event like that conference!</p>

<p>More specifically, I'll be telling you how <strong>I</strong> do it, some of the things I do are just for flair, and honestly a bit over the top. <g-emoji class="g-emoji" alias="telescope" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f609.png">ðŸ˜‰</g-emoji></p>

<hr/>
<h4>Summary</h4>
<ul>
  <li><a class='no-underline' routerLink='.' fragment='organizing-the-event'>Organizing the event</a></li>
  <li><a class='no-underline' routerLink='.' fragment='preparing-assets'>Preparing assets</a></li>
  <li><a class='no-underline' routerLink='.' fragment='the-day-of'>"The Day Of"</a></li>
  <li><a class='no-underline' routerLink='.' fragment='aftermath'>The aftermath</a></li>
</ul>
<hr/>

<h4 id='organizing-the-event'>Organizing the event</h4>

<p>The organization of the event usually starts a few months ahead of time, mostly around mid-December. That's when we try and set a date for the conference, as well as plan out the great "phases" that it will go through.</p>

<p>Generally, there are 4 phases to plan out:</p>

<ul>
  <li>Preparation</li>
  <li>Talent Acquisition</li>
  <li>Talent Mentoring | Asset Preparation</li>
  <li>Conference</li>
</ul>

<p>The Preparation phase is where we will lock dates, create the Call For Presentations form (CFP), find willing mentors for the Talent Mentoring phase, find people available to review the individual responses to the CFP, and finally attempt to have an idea about the stylistic sense of the conference.</p>

<p>The Talent Acquisition phase is where people will put in CFPs, and where reviewers take the time to review and grade the submissions. Where we differ from most other conferences, is that we don't ask a detailed CFP. All we need is a title, a short description and a motivation for giving the talk, which helps us root out corporate / vendor / snake oil talks. I usually don't involve myself with the CFP review process, as it'd be unfair to the participants.</p>

<p>The Talent Mentoring phase only concerns the speakers and mentors, and is happening parallel to the Asset Preparation phase. It mainly involves speakers working on their talks and mentors supporting them and being available for questions and such things.</p>

<p>The Asset Preparation phase is where people like myself generate the stream assets, such as images, videos, layouts, etc. as well as source the relevant music assets. I'll be going a bit more into that in the <a href='.' fragment='preparing-assets'><em class="fas fa-link"></em> next section</a>.</p>

<p>Then comes the conference, which is a 3-4 day period where we do setup and tests, followed by the conference, followed by the break down, including the upload of all the conference outputs to YouTube.</p>

<p>So that's a lot of phases with announcements, communication and the like to plan in advance.</p>

<br/>
<h4 id='preparing-assets'>Preparing assets</h4>

<p>There are several assets at play when it comes to the conference (see below for some sneak peeks)! They're usually a pain to create, using a variety of tools, such as GIMP, Blender VSE, FFMPEG.</p>

<p>There's the promotional assets, such as the poster for the conference, and the announcement posters!</p>

<div class='media-container'>
  <img  width='400' src='/assets/images/post/2022/220426-digital-overdose-conference2/conference_cover_2022.png' alt='The conference cover art.' />
  <span><em>The conference cover art.</em></span>
</div>

<p>Then there's the stream assets, such as the various layouts used in OBS.</p>

<div class='media-container'>
  <img  width='50%' src='/assets/images/post/2022/220426-digital-overdose-conference2/stream-asset.png' alt='Live shot of the stream assets in use, during a talk.' />
  <span><em>Live shot of the stream assets in use, during a talk.</em></span>
</div>

<div class='media-container'>
  <img  width='50%' src='/assets/images/post/2022/220426-digital-overdose-conference2/stream-asset-2.png' alt='Live shot of the stream assets in use, during a break.' />
  <span><em>Live shot of the stream assets in use, during a break.</em></span>
</div>

<p>Also to note are musical assets, which we source beforehand and need to integrate into the stream assets in order to make the conference smooth, and to keep the attention of the attendees.</p>

<div class='media-container'>
  <img width='50%' src='/assets/images/post/2022/220426-digital-overdose-conference2/TRACK01.gif' alt='10 second GIF of one of the audio assets.' />
  <span><em>10 second GIF of one of the audio assets.</em></span>
</div>

<p>How did I make that, you ask? (Or if you didn't, that's a shame, you're still finding out <g-emoji class="g-emoji" alias="telescope" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f609.png">ðŸ˜‰</g-emoji>)</p>

<p>Well the initial part is an FFMPEG script (below) that takes the MP3 and generates the waveform.</p>

<div class='d-flex media-container'>
  <markdown lineNumbers ngPreserveWhitespaces>
```bash
ffmpeg -y -i ./_track01_24k.wav -i ./trackbg_full.png -ar 96K -ss 00:00:00 -to 00:04:32 -pix_fmt yuv422p \
 -colorspace bt2020nc -color_primaries bt2020 -color_trc smpte2084 -dst_range 1 -src_range 1 -color_range 2 \
 -sws_flags +accurate_rnd+full_chroma_int+full_chroma_inp -sws_dither none -sar 1:1 -fflags +igndts -fflags \
 +genpts -vsync 1 -r 30 -filter_complex "[1:v]loop=loop=-1:start=0:size=450,boxblur=0:0:0:0:3:1,fps=30[img];
 [0:a]highpass=f=1,acompressor=level_in=8:threshold=0.125:detection=peak:ratio=4,acompressor=level_in=16:threshold=0.24:
 detection=peak:ratio=4,volume=2.2,showfreqs=colors=white:mode=bar:fscale=lin:ascale=lin:win_size=256:size=70x70,format=rgba,
 lagfun=decay=0.6:planes=1,fps=60,colorkey=color=0x00000000:similarity=0.01:blend=0,drawgrid=w=15:h=0:t=1:replace=1:c=0x00000000,
 colorkey=color=black,hue=H=t,split=2[x1][x2];[x1]vflip[x3];[x2][x3]vstack[v1];
 [v1]split=2[x4][x5];[x4]hflip[x6]; [x6][x5]hstack[v2]; [img][v2]overlay=474:20" __track01.nut
```
  </markdown>
</div>

<p>The second part is transforming the <code>.nut</code> file to a <code>.mp4</code> file, as follows:</p>

<div class='d-flex media-container'>
  <markdown lineNumbers ngPreserveWhitespaces>
```bash
ffmpeg -i ./__track01.nut -c:v copy -c:a aac __track01.mp4
```
  </markdown>
</div>

<p>The third part is creating a Blender VSE system that will render it. Now of course, having 25 tracks to do this to, we choose to automate everything. And it turns out Blender does accept Python for most of its context specific work!</p>

<div class='d-flex media-container'>
  <markdown lineNumbers ngPreserveWhitespaces>
```python
seq = bpy.context.scene.sequence_editor
for strip in seq.sequences:
  seq.sequences.remove(strip)

bg = seq.sequences.new_image(f'Background', filepath=f'/home/atomic/code/conference-resources/MUSIC_RESOURCES/trackbg_full.png', channel=1, frame_start=1)
mov = seq.sequences.new_movie(f'Audio Visualizer', filepath=f'/home/atomic/code/conference-resources/MUSIC_RESOURCES/__track{{ '{' }}str(t).zfill(2){{ '}' }}.mp4', channel=2, frame_start=1)
snd = seq.sequences.new_sound(f'Audio', filepath=f'/home/atomic/code/conference-resources/MUSIC_RESOURCES/_track{{ '{' }}str(t).zfill(2){{ '}' }}.mp3', channel=3, frame_start=1)
img = seq.sequences.new_image(f'Title', filepath=f'/home/atomic/code/conference-resources/MUSIC_RESOURCES/___track{{ '{' }}str(t).zfill(2){{ '}' }}.png', channel=4, frame_start=1)

duration = snd.frame_duration
bpy.context.scene.frame_end = duration
bg.frame_offset_end = -duration+1
img.frame_offset_end = -duration+1
mov.frame_offset_end = -duration+1

values = [0.0, 1.0, 1.0, 0.0]
snd_frames = [0, 5, duration-5, duration]
vis_frames = [0, 20, duration-20, duration]

for i in range(len(values)):
  snd.volume = values[i]
  snd.keyframe_insert('volume', frame=snd_frames[i])
  for channel in [img, mov]:
    channel.blend_alpha = values[i]
    channel.keyframe_insert('blend_alpha', frame=vis_frames[i])

bpy.context.scene.render.filepath = f'/home/atomic/code/conference-resources/MUSIC/TRACK{{ '{' }}str(t).zfill(2){{ '}' }}.mp4'
```
  </markdown>
</div>

<p>It may take a short while to set up, but this is actual time-saving automation. Not like this tweet echoing a programming meme would have you think:</p>

<div class='media-container'>
  <ngx-tweet loading="lazy" tweetId="1254266079532154880"></ngx-tweet>
</div>

<br/>
<h4 id='the-day-of'>"The Day Of"</h4>

<p>The Day Of is always a really chaotic time. Why? Because of Murphy's Law ("Everything that can go bad, will go bad"). This is why we have mitigations in place, and also avoid things getting bad!</p>

<p>All went well this time around, mainly due to the entire team working well together, and making sure the community was looked after.</p>

<p>But what did it look like on the ground?</p>

<p>Well, there are several issues when organizing virtual conferences. You need someone to do A/V. You need someone with a good internet connection. You need hosts. You need speakers.</p>

<p>If you look at the picture below, you realize that people need to handle more than one role at once.</p>

<div class='media-container'>
  <img  width='50%' src='/assets/images/post/2022/220426-digital-overdose-conference2/desk-setup.jpg' alt='A picture of my desk during the conference.' />
  <span><em>A picture of my desk during the conference.</em></span>
</div>

<p>Let me explain: What you see there is the entire A/V setup for the conference.</p>

<p>The upper left monitor is the OBS monitor. It handles streaming the content coming in from the lower left laptop, which hosts a Zoom session.</p>

<p>The upper right monitor is the Oversight monitor. That's where I look at comments, discussions, etc. whilst my own laptop (lower-center), joins the Zoom session.</p>

<p>The closed laptop to the right is the post-processing laptop. Any and all conference streams that we've recorded get post-processed as fast as possible, in order to be able to upload them by the time the conference finishes.</p>

<p>In the frame you'll also see a pair of comfortable headphones, an "OK" microphone, and my StreamDeck. These make running the conference a slightly smoother experience. For reference, last year, my phone running the OBS app was used instead of the Stream Deck, which was a pain to deal with. Same with the headphones, last year's headphones were less comfortable than the pair used this year.</p>

<p>Now, another question arises: Why wouldn't I get someone else to do A/V?</p>

<p>The answer to that is quite simple, that would either imply bringing someone to where I host the conference, or having someone available enough, with sufficient bandwidth, and a keen attention to detail there. Then they would have to send me the raw footage for post-processing. In my opinion, it's avoidable overhead, even though that means I have a bit more work to do.</p>

<br/>
<h4 id='aftermath'>The aftermath</h4>

<p>So, what happens after? Well, the post-processing is finished, and the videos are all promptly uploaded to the community's YouTube channel.</p>

<div class='media-container'>
  <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/videoseries?list=PLUI-ug97ALy38dw8-1pkMexj_7qpjEbkH" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

<p>Beyond that, the logistics for conference swag need to be put into place, in order to source stickers, t-shirts, and the like for our speakers.</p>

<p>Another thing we like to look at is the metrics. This isn't to see which talks performed better than others, but rather to see which timeslots where globally more attractive to our audience.</p>

<div class='media-container'>
  <img  width='50%' src='/assets/images/post/2022/220426-digital-overdose-conference2/summary-day-1.png' alt='Twitch stream statistics for the first day of the conference.' />
  <span><em>Twitch stream statistics for the first day of the conference.</em></span>
</div>

<div class='media-container'>
  <img  width='50%' src='/assets/images/post/2022/220426-digital-overdose-conference2/summary-day-2.png' alt='Twitch stream statistics for the second day of the conference.' />
  <span><em>Twitch stream statistics for the second day of the conference.</em></span>
</div>

<p>What can we find out? Well, for starters - and unsurprisingly, Sunday is slightly less favoured by viewers.</p>

<p>Lunch time coincides with a dip in attendance, which makes sense as the only thing playing was music.</p>

<p>People are actively chatting in both streams, so it isn't just background engagement we are getting, which is very interesting.</p>

<p>Finally, we can observe that the streams go on for quite a while, approximately 10 to 11 hours, which is a very long amount of time.</p>

<br/>
<p>So yeah, that was Digital Overdose Conference 2!</p>
</app-post-body>
